# URL utilisée par le gateway Python
LLM_URL=http://ollama:11434/v1/chat/completions
#
# Exemple provider alternatif (vLLM, LM Studio, OpenAI-compatible) :
# LLM_URL=http://127.0.0.1:1234/v1/chat/completions
# LLM_URL=https://api.openai.com/v1/chat/completions
# LLM_MODEL=gpt-4o-mini

# Modèle Ollama à charger (ex: qwen2.5-coder:14b, gemma2:9b, deepseek-coder)
LLM_MODEL=qwen2.5-coder:14b

# LLM_MODEL et LLM_URL sont indépendants de la chaîne image SDXL.

# Modèle/endpoint d’image (SDXL)
# Note: LLM_URL et LLM_MODEL sont indépendants de l’endpoint image SDXL.

# Image generation provider
# image url doit pointer vers une API SDXL compatible (ex: /sdapi/v1/txt2img pour Automatic1111).
SDXL_IMAGE=runpod/stable-diffusion-webui:latest
SDXL_URL=http://sdxl:7860/sdapi/v1/txt2img
SDXL_MODEL=stabilityai/stable-diffusion-xl-base-1.0
SDXL_PROVIDER=auto
SDXL_TIMEOUT_SEC=180

# Ports exposés
OLLAMA_HOST_PORT=11434
STUDIO_AI_HOST_PORT=8787
SDXL_HOST_PORT=7860
